ALGORITHM AND CONCLUSIONS:
    This uses dynamic programming.
    The value function is written using Bellman optimality equation V[s]=max(p_h*V[s+a]+(1-p_h)*V[s-a]) for actions a.
    The optimal policy is obtained by choosing the greedy action with respect to the final value function.
    The value function is stored after each sweep to visualize convergence as told in the example problem.
    As p_h increases, the winning is likely hence the value function to rise sharply near the goal and the policy chooses maximum stake.

